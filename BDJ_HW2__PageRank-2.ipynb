{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BDJ_HW2_ PageRank.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2QKFIlR6HCY"
      },
      "source": [
        "# PageRank\n",
        "\n",
        "In this assignment, you will compute PageRank on a collection of 469,235 web sites using the iterative version of the PageRank algorithm described in class for sparse graphs (NOT the power method with explicit matrix multiplication).\n",
        "\n",
        "Consider the following directed graph:\n",
        "\n",
        "![A directed link graph](https://ccs.neu.edu/home/dasmith/courses/cs6200/pagerank.jpg)\n",
        "\n",
        "We can represent this graph as a collection of nodes, here, ordered pairs of node index and node name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D_Mxj5pXKPl"
      },
      "source": [
        "small_nodes = [(0, 'A'),\n",
        "              (1, 'B'),\n",
        "              (2, 'C'),\n",
        "              (3, 'D'),\n",
        "              (4, 'E'),\n",
        "              (5, 'F')]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTF3JKtTYxiZ"
      },
      "source": [
        "and a collection of directed links, i.e., ordered pairs from source to target:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i0V5ueOYDDN"
      },
      "source": [
        "small_edges = [\n",
        "  (0, 1),\n",
        "  (0, 2),\n",
        "  (0, 5),\n",
        "  (1, 2),\n",
        "  (1, 3),\n",
        "  (1, 4),\n",
        "  (1, 5),\n",
        "  (2, 3),\n",
        "  (2, 4),\n",
        "  (3, 0),\n",
        "  (3, 2),\n",
        "  (3, 4),\n",
        "  (3, 5),\n",
        "  (4, 0),\n",
        "  (5, 0),\n",
        "  (5, 1),\n",
        "  (5, 4)\n",
        "]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sink_nodes = []\n",
        "for node in small_nodes:\n",
        "\n",
        "      Q = []\n",
        "      #Creating a list with all the edge node\n",
        "      for edge in small_edges:\n",
        "        \n",
        "        if(edge[0] == node[0] ):\n",
        "          Q.append(edge[1])\n",
        "      sink_nodes.append (Q)\n",
        "\n",
        "sink_nodes[:10]  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IZj691fPzoW",
        "outputId": "b2f86545-3dcb-419c-dc82-1efe6da55ebf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 5], [2, 3, 4, 5], [3, 4], [0, 2, 4, 5], [0], [0, 1, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBVDeszXY4B_"
      },
      "source": [
        "We use integer identifiers for the nodes for efficiency. Note that, unlike this example, in a real web graph, not every page will have in-links, nor will every page have out-links."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPNsTGSsEwMX"
      },
      "source": [
        "## First Implementation and Test\n",
        "\n",
        "\\[10 points\\] Implement the iterative PageRank algorithm. Test your code on the six-node example using the input representation given above.  Be sure that your code handles pages that have no in-links or out-links properly.  (You may wish to test on a few such examples.) In later parts of this assignment, depending on how you store the data, it may be convenient to use iterators rather than storing the data in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMu_WaDA55sk"
      },
      "source": [
        "# TODO: Implement PageRank, given nodes and edges, to start with a uniform\n",
        "# distribution over nodes, run a fixed number of iterations, and\n",
        "# return a distribution over nodes.\n",
        "\n",
        "def page_rank_fixed_iter(nodes, edges, iterations=10):\n",
        "\n",
        "  I = []\n",
        "\n",
        "  #Initialize page rank for all pages to be equally likely.\n",
        "  for i in range (0,len(nodes)):\n",
        "    I.append(1/len(nodes))\n",
        "\n",
        "  for x in range (0,iterations):\n",
        "    lamda = 0.85\n",
        "\n",
        "    R = []\n",
        "    #Assigning random page rank value\n",
        "    for i in range (0,len(nodes)):\n",
        "      R.append(lamda/len(nodes))\n",
        "\n",
        "    #Updating the resulting better page rank value\n",
        "    for node_list in nodes:\n",
        "\n",
        "      Q = []\n",
        "      #Creating a list with all the edge node\n",
        "      for edge in edges:\n",
        "        \n",
        "        if(edge[0] == node_list[0] ):\n",
        "          Q.append(edge[1])\n",
        "\n",
        "      \n",
        "      p = node_list[0]\n",
        "      if(len(Q)>0):\n",
        "        for q in Q:\n",
        "          R[q] = R[q] + (((1-lamda)*I[p])/len(Q))\n",
        "      else:\n",
        "        for node in nodes:\n",
        "          p = node_list[0]\n",
        "          R[node[0]] = R[node[0]] + (((1-lamda)*I[p])/len(nodes))\n",
        "\n",
        "\n",
        "      #Update Value of R to I\n",
        "    for i in range(0,len(R)):\n",
        "      I[i] = R[i]\n",
        "\n",
        "  final_node_list = []\n",
        "\n",
        "  count = 0\n",
        "  for node in small_nodes:\n",
        "    list1 = []\n",
        "    list1.append(node[0])\n",
        "    list1.append(node[1])\n",
        "    list1.append(R[count])\n",
        "    final_node_list.append(list1)\n",
        "    count = count +1\n",
        "\n",
        "  print(\"Sum of all Page rank values:\")\n",
        "  print(sum(R))  \n",
        "  return final_node_list\n",
        "\n",
        "\n",
        "# Output PageRank on the toy graph at various points.\n",
        "# Make sure your output has node number, name, and PageRank value."
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_rank_fixed_iter(small_nodes, small_edges, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqLf_UbPppjA",
        "outputId": "f618245d-9e04-4354-8758-32ec75cc8290"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of all Page rank values:\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 'A', 0.18125],\n",
              " [1, 'B', 0.15833333333333333],\n",
              " [2, 'C', 0.1625],\n",
              " [3, 'D', 0.16041666666666668],\n",
              " [4, 'E', 0.17500000000000002],\n",
              " [5, 'F', 0.1625]]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page_rank_fixed_iter(small_nodes, small_edges, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXTXH_T2p8gT",
        "outputId": "9b25fdb8-b208-4f90-a3d6-b5e96668bc7e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of all Page rank values:\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 'A', 0.1818896215995887],\n",
              " [1, 'B', 0.1588968156618608],\n",
              " [2, 'C', 0.1627133583012305],\n",
              " [3, 'D', 0.1598287991268914],\n",
              " [4, 'E', 0.17395804700919812],\n",
              " [5, 'F', 0.1627133583012305]]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page_rank_fixed_iter(small_nodes, small_edges, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8NkbX_vksTr",
        "outputId": "6ee7050e-cbc0-4f3f-a4d8-79e5fbd5d45c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of all Page rank values:\n",
            "1.0000000000000002\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 'A', 0.18188962160030808],\n",
              " [1, 'B', 0.15889681566174427],\n",
              " [2, 'C', 0.16271335830124406],\n",
              " [3, 'D', 0.15982879912657538],\n",
              " [4, 'E', 0.17395804700888418],\n",
              " [5, 'F', 0.16271335830124406]]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4duRjzABB9n"
      },
      "source": [
        "## PageRank on Web Crawl Data\n",
        "\n",
        "\\[20 points\\] Download and unpack a list of `.edu` websites and the links among them from the [Common Crawl](https://commoncrawl.org/2017/05/hostgraph-2017-feb-mar-apr-crawls/) open-source web crawl. For the sake of brevity, the data record links among websites, not web pages. The information for nodes and links is the same as the toy example above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6EDDdTQCd3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e547afec-e9df-4198-df03-4ca52ffaad71"
      },
      "source": [
        "# If you're running on a machine (e.g., Windows) that doesn't have wget or gzip,\n",
        "# feel free to comment this out and use a different set of commands to load\n",
        "# the data.\n",
        "!wget https://ccs.neu.edu/home/dasmith/courses/cs6200/vertices-edu.txt.gz\n",
        "!gzip -df vertices-edu.txt.gz\n",
        "!wget https://ccs.neu.edu/home/dasmith/courses/cs6200/edges-edu.txt.gz\n",
        "!gzip -df edges-edu.txt.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-23 19:32:13--  https://ccs.neu.edu/home/dasmith/courses/cs6200/vertices-edu.txt.gz\n",
            "Resolving ccs.neu.edu (ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to ccs.neu.edu (ccs.neu.edu)|52.70.229.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3703486 (3.5M) [application/x-gzip]\n",
            "Saving to: ‘vertices-edu.txt.gz’\n",
            "\n",
            "vertices-edu.txt.gz 100%[===================>]   3.53M  2.79MB/s    in 1.3s    \n",
            "\n",
            "2022-02-23 19:32:15 (2.79 MB/s) - ‘vertices-edu.txt.gz’ saved [3703486/3703486]\n",
            "\n",
            "--2022-02-23 19:32:16--  https://ccs.neu.edu/home/dasmith/courses/cs6200/edges-edu.txt.gz\n",
            "Resolving ccs.neu.edu (ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to ccs.neu.edu (ccs.neu.edu)|52.70.229.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12829526 (12M) [application/x-gzip]\n",
            "Saving to: ‘edges-edu.txt.gz’\n",
            "\n",
            "edges-edu.txt.gz    100%[===================>]  12.23M  6.76MB/s    in 1.8s    \n",
            "\n",
            "2022-02-23 19:32:18 (6.76 MB/s) - ‘edges-edu.txt.gz’ saved [12829526/12829526]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW4yp1gPUwzb"
      },
      "source": [
        "There should now be files `vertices-edu.txt` and `edges-edu.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly1t9fyjK7eC"
      },
      "source": [
        "# TODO: Process the raw data into the same format as the simple graph.\n",
        "# You may create lists or iterators."
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# opening the vertices/nodes file in read mode\n",
        "f = open(\"vertices-edu.txt\", \"r\")\n",
        "\n",
        "#Creating List of Lists with Page number and Name \n",
        "nodes_list = []\n",
        "for line in f.readlines():\n",
        "    fields = line.split(' ')\n",
        "    temp = []\n",
        "    temp.append(int(fields[0]))\n",
        "    temp.append(fields[1])\n",
        "    nodes_list.append(temp)\n",
        "f.close()\n",
        "\n",
        "print(\"Processed raw data into list for the Nodes/vertices: \")\n",
        "print(nodes_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJymtAZN14rp",
        "outputId": "b7fdc14a-fce1-4a0a-fbcc-8b0588e9afea"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed raw data into list for the Nodes/vertices: \n",
            "[[0, 'edu.00zl5e\\n'], [1, 'edu.06hxbt\\n'], [2, 'edu.082ifc\\n'], [3, 'edu.083mjs\\n'], [4, 'edu.09xzrr\\n'], [5, 'edu.0aoqqj\\n'], [6, 'edu.0ax4el\\n'], [7, 'edu.0c5fez\\n'], [8, 'edu.0cosn2\\n'], [9, 'edu.0dcdp8\\n']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opening the edges file in read mode\n",
        "f = open(\"edges-edu.txt\", \"r\")\n",
        "\n",
        "#Creating List of Lists with Page number and Name \n",
        "edges_list = []\n",
        "for line in f.readlines():\n",
        "    fields = line.split(' ')\n",
        "    temp = []\n",
        "    temp.append(int(fields[0]))\n",
        "    temp.append(int(fields[1]))\n",
        "    edges_list.append(temp)\n",
        "f.close()\n",
        "\n",
        "print(\"Processed raw data into list for the Edges: \")\n",
        "print(edges_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Di_9mnA2zZC",
        "outputId": "fe6be5db-89c2-4bc6-ca46-8be181d95149"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed raw data into list for the Edges: \n",
            "[[386, 440], [19202, 1033], [103884, 2635], [342306, 7399], [8366, 8312], [8358, 8312], [8949, 8987], [8982, 8987], [8910, 8987], [9028, 8999]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of Inlinks\n",
        "from collections import defaultdict\n",
        "  \n",
        "# Grouping list values into dictionary\n",
        "# Using defaultdict() + loop + dict()\n",
        "sink = defaultdict(list)\n",
        "for key, val in edges_list:\n",
        "    sink[key].append(val)\n",
        "res = dict((key, tuple(val)) for key, val in sink.items())\n",
        "\n",
        "\n",
        "print(list(res.items())[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQbrGQq7VA0Z",
        "outputId": "6c557e5f-8155-4c0f-a747-52e84f9f3194"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(386, (440, 436, 435477, 35740, 25778, 433, 394, 322170, 410, 88948, 420, 419, 206678, 389, 304453, 392, 425, 405, 215476, 402, 437, 377768, 436406, 70065, 398, 315, 444, 424, 32917, 446, 395, 3451, 439, 426, 441, 89292, 153823, 423, 393, 428, 241269, 388, 442, 415, 621, 19458, 411, 406, 432, 129460, 652, 413, 176740, 437350, 154980, 387, 431, 443, 429, 396, 19201, 421, 56176, 404, 407, 427, 417, 438, 119189, 312074, 310453, 445, 170227, 412, 2129)), (19202, (1033, 19225, 71095, 168518, 55038, 26503, 36294, 460809, 19224, 93874, 65979, 170300, 148945, 101785, 122284, 19209, 188542, 19204, 19232, 19221, 78641, 19206, 247831, 455665, 19236, 457249, 11064, 19231, 168782, 140443, 206605, 197272, 19218, 171975, 19207, 456986, 13154, 13023, 149173, 19226, 19220, 19213, 130943, 170448, 19217, 91072, 19219, 309622, 368458, 366878, 19205))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sink_nodes = []\n",
        "\n",
        "#Creating a dictionary having the page number and its in-link count\n",
        "to_links_list = [element[1] for element in edges_list]\n",
        "import itertools \n",
        "    \n",
        "def CountFrequency(my_list):\n",
        " \n",
        "    # Creating an empty dictionary\n",
        "    count = {}\n",
        "    for i in my_list:\n",
        "     count[i] = count.get(i, 0) + 1\n",
        "    return count\n",
        "\n",
        "dict1 = CountFrequency(to_links_list)\n",
        "\n",
        "# Creating the in-link list with number, id, in-link count\n",
        "in_link_list = []\n",
        "for node in nodes_list:\n",
        "  temp = []\n",
        "  temp.append(node[0])\n",
        "  temp.append(node[1])\n",
        "  if node[0] in dict1:\n",
        "    temp.append(dict1.get(node[0]))\n",
        "  else:\n",
        "    temp.append(0)\n",
        "  in_link_list.append(temp)\n",
        "\n",
        "for node in in_link_list:\n",
        "  if  node[2] == 0:\n",
        "    sink_nodes.append(node[0])\n"
      ],
      "metadata": {
        "id": "__ByRYuFeecG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P = []\n",
        "for node in nodes_list:\n",
        "  P.append(node[0])\n",
        "total_nodes = len(P)  \n",
        "\n",
        "print(sink_nodes[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt9RwijmKWBd",
        "outputId": "9a8a58d7-9f25-4a5d-91de-1714ef4222fc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Creating a dictionary having the page number and its out-link count\n",
        "from_links_list = [element[0] for element in edges_list]\n",
        "import itertools \n",
        "    \n",
        "def CountFrequency(my_list):\n",
        " \n",
        "    # Creating an empty dictionary\n",
        "    count = {}\n",
        "    for i in my_list:\n",
        "     count[i] = count.get(i, 0) + 1\n",
        "    return count\n",
        "\n",
        "dict2 = CountFrequency(from_links_list)\n",
        "\n",
        "# Creating the in-link list with number, id, in-link count\n",
        "out_link_list = []\n",
        "for node in nodes_list:\n",
        "  temp = []\n",
        "  temp.append(node[0])\n",
        "  if node[0] in dict2:\n",
        "    temp.append(dict2.get(node[0]))\n",
        "  else:\n",
        "    temp.append(0)\n",
        "  out_link_list.append(temp)\n"
      ],
      "metadata": {
        "id": "ZWGZHkipfuPC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WMf5L5VEqZb"
      },
      "source": [
        "Refine your implementation of PageRank to test for numerical convergence. Specificially, at each iteration, calculate the [perplexity](https://en.wikipedia.org/wiki/Perplexity) of the PageRank distribution, where perplexity is defined as 2 raised to the [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of the PageRank distribution, i.e., $2^{H(PR)}$. (Recall that we defined entropy when talking about data compression.) The maximum perplexity of a PageRank distribution will therefore be the number of nodes in the graph.\n",
        "\n",
        "At each iteration, check the _change_ in perplexity. If the change is less than some threshold, you can stop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0yQKvKqAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc4dee89-e958-400c-cf8a-ff6151b2adb7"
      },
      "source": [
        "# TODO: Implement convergence testing in PageRank\n",
        "# If you choose, you can share some subroutines with your first version.\n",
        "# Print the change in perplexity at each iteration.\n",
        "\n",
        "import math\n",
        "\n",
        "# Creating a Dictionary of Source Node: List of Sink nodes\n",
        "from collections import defaultdict\n",
        "  \n",
        "# Grouping list values into dictionary\n",
        "# Using defaultdict() + loop + dict()\n",
        "sink = defaultdict(list)\n",
        "for key, val in edges_list:\n",
        "    sink[key].append(val)\n",
        "res = dict((key, tuple(val)) for key, val in sink.items())\n",
        "\n",
        "def calculate_perplexity(pageRank):\n",
        "  sum = 0\n",
        "  for i in range(0,len(pageRank)):\n",
        "    sum += pageRank[i]*math.log2(pageRank[i])\n",
        "  return 2**sum;\n",
        "\n",
        "\n",
        "def page_rank(nodes, edges, threshold=1):\n",
        "\n",
        "  I = []\n",
        "\n",
        "  #Initialize page rank for all pages to be equally likely.\n",
        "  for i in range (0,len(nodes)):\n",
        "    I.append(1/len(nodes))\n",
        "\n",
        "  \n",
        "  old_perplexity = 0\n",
        "  perplexity_change = 0\n",
        "  iteration = 1\n",
        "  while True:\n",
        "    lamda = 0.85\n",
        "\n",
        "    R = []\n",
        "    #Assigning random page rank value\n",
        "    for i in range (0,len(nodes)):\n",
        "      R.append((lamda)/len(nodes))\n",
        "\n",
        "    #Handling sinks in the Graph  \n",
        "    total_sink_PR = 0\n",
        "    for sink_node in sink_nodes:\n",
        "      total_sink_PR += I[sink_node]\n",
        "\n",
        "\n",
        "    #Updating the resulting better page rank value\n",
        "    for node in nodes:\n",
        "\n",
        "      Q = []\n",
        "      #Creating a list with all the edge node\n",
        "      if node[0] in res:\n",
        "        Q = list(res.get(node[0]))\n",
        "      \n",
        "      R[node[0]] += ( (1-lamda) * total_sink_PR/total_nodes )\n",
        "\n",
        "      for q in Q:\n",
        "        R[q] += (1-lamda) * I[node[0]]/len(Q)\n",
        "\n",
        "\n",
        "    new_perplexity = calculate_perplexity(R)\n",
        "    perplexity_change = abs(new_perplexity - old_perplexity)\n",
        "    old_perplexity = new_perplexity\n",
        "    print(\"Perplexity change of the Iteration \" + str(iteration) + \"   :  \" + str(perplexity_change))\n",
        "    iteration += 1\n",
        "      #Update Value of R to I\n",
        "    for i in range(0,len(R)):\n",
        "      I[i] = R[i]\n",
        "\n",
        "    print(\"Sum of PR = \" + str(sum(R)))\n",
        "    if perplexity_change < threshold:\n",
        "      break\n",
        "\n",
        "  return I\n",
        "\n",
        "# Run until perplexity changes by less than 1\n",
        "PR = page_rank(nodes_list, edges_list, 0.5)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity change of the Iteration 1   :  4.155329051596602e-06\n",
            "Sum of PR = 0.9476308246414972\n",
            "Perplexity change of the Iteration 2   :  1.654109404552222e-07\n",
            "Sum of PR = 0.9442915819311075\n",
            "Perplexity change of the Iteration 3   :  1.6607613529197768e-08\n",
            "Sum of PR = 0.9439668015142575\n",
            "Perplexity change of the Iteration 4   :  1.6834950573325615e-09\n",
            "Sum of PR = 0.943933019506807\n",
            "Perplexity change of the Iteration 5   :  1.9970678511315208e-10\n",
            "Sum of PR = 0.9439290370416838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_PR_list = []\n",
        "\n",
        "count = 0\n",
        "for node in nodes_list:\n",
        "    list1 = []\n",
        "    list1.append(node[0])\n",
        "    list1.append(node[1])\n",
        "    list1.append(PR[count])\n",
        "    final_PR_list.append(list1)\n",
        "    count = count +1\n"
      ],
      "metadata": {
        "id": "ZzjYlcOnd2L5"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcncY2QHNl0M"
      },
      "source": [
        "## Link Analysis\n",
        "\n",
        "\\[20 points\\] In this final section, you will compute some properties of the web-site graph you downloaded.\n",
        "\n",
        "First, consider the _in-link count_ of a website, simply the number of web-sites pointing to it (including self-links). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_FyPlLSO2bu"
      },
      "source": [
        "# TODO: List the document ID, domain name, and in-link count of the 60 websites with the highest in-link count"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a dictionary having the page number and its in-link count\n",
        "to_links_list = [element[1] for element in edges_list]\n",
        "import itertools \n",
        "    \n",
        "def CountFrequency(my_list):\n",
        " \n",
        "    # Creating an empty dictionary\n",
        "    count = {}\n",
        "    for i in my_list:\n",
        "     count[i] = count.get(i, 0) + 1\n",
        "    return count\n",
        "\n",
        "dict1 = CountFrequency(to_links_list)\n",
        "\n",
        "# Creating the in-link list with number, id, in-link count\n",
        "in_link_list = []\n",
        "for node in nodes_list:\n",
        "  temp = []\n",
        "  temp.append(node[0])\n",
        "  temp.append(node[1])\n",
        "  if node[0] in dict1:\n",
        "    temp.append(dict1.get(node[0]))\n",
        "  else:\n",
        "    temp.append(0)\n",
        "  in_link_list.append(temp)\n",
        "\n",
        "\n",
        "#Sorting the list for top 60 items\n",
        "print(\"Printing the document ID, domain name, and in-link count of the 60 websites with the highest in-link count\")\n",
        "from operator import itemgetter\n",
        "sorted_in_link_list = sorted(in_link_list, key=itemgetter(2), reverse = True)\n",
        "sorted_in_link_list[0:60]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT-01X3wDP1G",
        "outputId": "984aef2f-2206-4a2f-a275-730bd998f897"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing the document ID, domain name, and in-link count of the 60 websites with the highest in-link count\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[185524, 'edu.mit.web\\n', 4388],\n",
              " [278032, 'edu.stanford\\n', 4021],\n",
              " [244433, 'edu.purdue.english.owl\\n', 3531],\n",
              " [140443, 'edu.indiana\\n', 3339],\n",
              " [237176, 'edu.princeton\\n', 3251],\n",
              " [64587, 'edu.columbia\\n', 3123],\n",
              " [465503, 'edu.yale\\n', 2804],\n",
              " [418623, 'edu.utexas\\n', 2622],\n",
              " [383763, 'edu.unc\\n', 2592],\n",
              " [197698, 'edu.nap\\n', 2494],\n",
              " [439637, 'edu.washington\\n', 2291],\n",
              " [373442, 'edu.umich\\n', 2281],\n",
              " [440674, 'edu.washington.depts\\n', 2276],\n",
              " [148945, 'edu.jhu.muse\\n', 2255],\n",
              " [60975, 'edu.colorado\\n', 2232],\n",
              " [449738, 'edu.wisc\\n', 2230],\n",
              " [38320, 'edu.bu\\n', 2205],\n",
              " [83572, 'edu.dartmouth\\n', 1965],\n",
              " [408380, 'edu.usc\\n', 1952],\n",
              " [178879, 'edu.mit\\n', 1946],\n",
              " [27307, 'edu.berkeley\\n', 1908],\n",
              " [233405, 'edu.pitt\\n', 1857],\n",
              " [191069, 'edu.msu\\n', 1810],\n",
              " [326371, 'edu.uchicago.press\\n', 1763],\n",
              " [136464, 'edu.illinois\\n', 1753],\n",
              " [93874, 'edu.educause\\n', 1741],\n",
              " [56979, 'edu.cmu.cs\\n', 1730],\n",
              " [199032, 'edu.ncsu\\n', 1709],\n",
              " [36294, 'edu.brown\\n', 1702],\n",
              " [202182, 'edu.nd\\n', 1689],\n",
              " [68675, 'edu.cornell\\n', 1685],\n",
              " [71095, 'edu.cornell.law\\n', 1646],\n",
              " [183214, 'edu.mit.mitpress\\n', 1644],\n",
              " [215627, 'edu.nyu\\n', 1625],\n",
              " [56538, 'edu.cmu\\n', 1583],\n",
              " [239378, 'edu.psu\\n', 1541],\n",
              " [350412, 'edu.ufl\\n', 1533],\n",
              " [120819, 'edu.harvard\\n', 1529],\n",
              " [270369, 'edu.si\\n', 1513],\n",
              " [107916, 'edu.gatech\\n', 1500],\n",
              " [365396, 'edu.uky\\n', 1497],\n",
              " [337138, 'edu.ucop\\n', 1482],\n",
              " [358246, 'edu.uic\\n', 1472],\n",
              " [382564, 'edu.umn.www1\\n', 1470],\n",
              " [403069, 'edu.upenn\\n', 1464],\n",
              " [293521, 'edu.tamu\\n', 1452],\n",
              " [284517, 'edu.stanford.web\\n', 1451],\n",
              " [256613, 'edu.rutgers\\n', 1440],\n",
              " [367316, 'edu.umass\\n', 1430],\n",
              " [457936, 'edu.wsu\\n', 1419],\n",
              " [36154, 'edu.brookings\\n', 1388],\n",
              " [323918, 'edu.uchicago\\n', 1377],\n",
              " [440902, 'edu.washington.faculty\\n', 1363],\n",
              " [282555, 'edu.stanford.plato\\n', 1353],\n",
              " [392894, 'edu.universityofcalifornia\\n', 1353],\n",
              " [329686, 'edu.ucla\\n', 1347],\n",
              " [317828, 'edu.ucdavis\\n', 1343],\n",
              " [354337, 'edu.uga\\n', 1339],\n",
              " [225417, 'edu.osu\\n', 1322],\n",
              " [393138, 'edu.unl\\n', 1319]]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uSlQEtmPTTA"
      },
      "source": [
        "Then, use the PageRank values compute by your second implementation. Note that some websites will have both a high in-link count and PageRank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwcci2kdPlMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f9b701a-0fdc-4989-f746-e5e2647102fe"
      },
      "source": [
        "# TODO: List the document ID, domain name, and PageRank of the 60 websites with the highest PageRank.\n",
        "\n",
        "from operator import itemgetter\n",
        "sorted_page_rank_list = sorted(final_PR_list, key=itemgetter(2), reverse = True)\n",
        "sorted_page_rank_list[0:60]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[185524, 'edu.mit.web\\n', 0.0001322568914268047],\n",
              " [284517, 'edu.stanford.web\\n', 9.639672306437066e-05],\n",
              " [278032, 'edu.stanford\\n', 9.155322685953744e-05],\n",
              " [465503, 'edu.yale\\n', 8.484000503403639e-05],\n",
              " [237176, 'edu.princeton\\n', 7.546154512912272e-05],\n",
              " [346969, 'edu.ucsf\\n', 7.406378369325191e-05],\n",
              " [60975, 'edu.colorado\\n', 7.230225840043798e-05],\n",
              " [136464, 'edu.illinois\\n', 7.211125726703664e-05],\n",
              " [307291, 'edu.ttu.depts\\n', 6.76228786581458e-05],\n",
              " [334739, 'edu.uconn\\n', 6.744108136568187e-05],\n",
              " [281817, 'edu.stanford.med\\n', 6.54614142860678e-05],\n",
              " [14945, 'edu.arizona\\n', 6.441472115957816e-05],\n",
              " [416576, 'edu.utah.lib.ezproxy.login\\n', 6.305179100783939e-05],\n",
              " [227645, 'edu.ou\\n', 6.080242858274807e-05],\n",
              " [408380, 'edu.usc\\n', 5.896224507322825e-05],\n",
              " [383763, 'edu.unc\\n', 5.827678718800397e-05],\n",
              " [449738, 'edu.wisc\\n', 5.804049177584484e-05],\n",
              " [191069, 'edu.msu\\n', 5.6290164821757145e-05],\n",
              " [68675, 'edu.cornell\\n', 5.565802014103293e-05],\n",
              " [239378, 'edu.psu\\n', 5.556913930390722e-05],\n",
              " [64587, 'edu.columbia\\n', 5.483368229655059e-05],\n",
              " [153823, 'edu.kit\\n', 5.383916658741527e-05],\n",
              " [341211, 'edu.ucsc\\n', 4.963444313235278e-05],\n",
              " [445554, 'edu.wesleyan\\n', 4.954807308141262e-05],\n",
              " [140443, 'edu.indiana\\n', 4.936153849525406e-05],\n",
              " [317828, 'edu.ucdavis\\n', 4.932378075649939e-05],\n",
              " [439637, 'edu.washington\\n', 4.9128286983790384e-05],\n",
              " [233405, 'edu.pitt\\n', 4.898163840054565e-05],\n",
              " [293521, 'edu.tamu\\n', 4.895122875869246e-05],\n",
              " [373442, 'edu.umich\\n', 4.870529257564243e-05],\n",
              " [342997, 'edu.ucsd\\n', 4.860115694823394e-05],\n",
              " [418623, 'edu.utexas\\n', 4.839881027168564e-05],\n",
              " [314976, 'edu.uark.library\\n', 4.838916486563518e-05],\n",
              " [358246, 'edu.uic\\n', 4.837346149809292e-05],\n",
              " [240779, 'edu.psu.extension\\n', 4.797234934165878e-05],\n",
              " [33074, 'edu.binghamton\\n', 4.793775713290674e-05],\n",
              " [107916, 'edu.gatech\\n', 4.735047863100512e-05],\n",
              " [124606, 'edu.harvard.hul.ezp-prod1\\n', 4.590134655886234e-05],\n",
              " [244433, 'edu.purdue.english.owl\\n', 4.589339542883923e-05],\n",
              " [323918, 'edu.uchicago\\n', 4.57407897934808e-05],\n",
              " [202182, 'edu.nd\\n', 4.4124095671248206e-05],\n",
              " [42252, 'edu.byu\\n', 4.3364462512055436e-05],\n",
              " [225417, 'edu.osu\\n', 4.334788730260756e-05],\n",
              " [377158, 'edu.umich.umd.wizard\\n', 4.2481743421982e-05],\n",
              " [394942, 'edu.unlv\\n', 4.240796704998113e-05],\n",
              " [38320, 'edu.bu\\n', 4.229713292828759e-05],\n",
              " [256613, 'edu.rutgers\\n', 4.20383353693563e-05],\n",
              " [27307, 'edu.berkeley\\n', 4.139443650763826e-05],\n",
              " [56538, 'edu.cmu\\n', 4.135902063109892e-05],\n",
              " [440674, 'edu.washington.depts\\n', 4.12100152701974e-05],\n",
              " [46450, 'edu.caltech\\n', 4.074907265190001e-05],\n",
              " [178879, 'edu.mit\\n', 3.969091519204857e-05],\n",
              " [459977, 'edu.wustl\\n', 3.945471928254076e-05],\n",
              " [120819, 'edu.harvard\\n', 3.9343400274724416e-05],\n",
              " [369944, 'edu.umd\\n', 3.934276915450608e-05],\n",
              " [284248, 'edu.stanford.tools\\n', 3.907104485654098e-05],\n",
              " [250831, 'edu.rit\\n', 3.892527373807167e-05],\n",
              " [38652, 'edu.bu.ezproxy\\n', 3.877482235130163e-05],\n",
              " [354337, 'edu.uga\\n', 3.7877141332386336e-05],\n",
              " [386196, 'edu.unc.web\\n', 3.724750444739763e-05]]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxODBxL_Pyy2"
      },
      "source": [
        "Finally, compute some summary statistics on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD4bq6AyQIsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68a8f2e-234f-453a-80f2-86aa97d70113"
      },
      "source": [
        "# TODO: Compute:\n",
        "# - the proportion of websites with no in-links (i.e., source nodes);\n",
        "\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "  \n",
        "count_no_in_links = Counter(chain.from_iterable(set(i) for i in in_link_list))[0]\n",
        "print(count_no_in_links)\n",
        "print(\"The proportion of websites with no in-links: \")\n",
        "count_total_in_links = len(nodes_list)\n",
        "proportion = count_no_in_links/count_total_in_links\n",
        "proportion\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "122722\n",
            "The proportion of websites with no in-links: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26153633041013563"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# - the proportion of websites with no out-links (i.e., sink nodes);\n",
        "\n",
        "#Creating a dictionary having the page number and its out-link count\n",
        "from_links_list = [element[0] for element in edges_list]\n",
        "import itertools \n",
        "    \n",
        "def CountFrequency(my_list):\n",
        " \n",
        "    # Creating an empty dictionary\n",
        "    count = {}\n",
        "    for i in my_list:\n",
        "     count[i] = count.get(i, 0) + 1\n",
        "    return count\n",
        "\n",
        "dict2 = CountFrequency(from_links_list)\n",
        "\n",
        "# Creating the in-link list with number, id, in-link count\n",
        "out_link_list = []\n",
        "for node in nodes_list:\n",
        "  temp = []\n",
        "  temp.append(node[0])\n",
        "  temp.append(node[1])\n",
        "  if node[0] in dict2:\n",
        "    temp.append(dict2.get(node[0]))\n",
        "  else:\n",
        "    temp.append(0)\n",
        "  out_link_list.append(temp)\n",
        "\n",
        "\n",
        "count_no_out_links = Counter(chain.from_iterable(set(i) for i in out_link_list))[0]\n",
        "print(count_no_out_links)\n",
        "print(\"The proportion of websites with no out-links: \")\n",
        "count_total_out_links = len(nodes_list)\n",
        "proportion2 = count_no_out_links/count_total_out_links\n",
        "proportion2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLJiabqCTKnH",
        "outputId": "34fd53a9-d636-42af-a2b8-98bb7f8eb7f0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "286545\n",
            "The proportion of websites with no out-links: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6106641661427643"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# - the proportion of websites whose PageRank is higher than the initial uniform distribution.\n",
        "\n",
        "initial_rank = 1/(len(nodes_list))\n",
        "count_initial_rank = 0\n",
        "\n",
        "for node in final_PR_list:\n",
        "  if node[2] > initial_rank:\n",
        "    count_initial_rank += 1\n",
        "\n",
        "total_count = len(nodes_list)\n",
        "proportion = count_initial_rank/total_count\n",
        "print(\" The proportion of websites whose PageRank is higher than the initial uniform distribution. \")\n",
        "print(proportion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSOPnm11Svfv",
        "outputId": "bdc0a067-a30c-4fe0-faa7-825a1220bc1a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The proportion of websites whose PageRank is higher than the initial uniform distribution. \n",
            "0.09556618751798139\n"
          ]
        }
      ]
    }
  ]
}